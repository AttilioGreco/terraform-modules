#cloud-config

write_files:
  - content: |
      - name: copy master mysql data
        hosts: 127.0.0.1
        pre_tasks:
          - block:
              - name: copy private ssh key
                copy:
                  dest: /root/.ssh/id_rsa
                  mode: 0400
                  content: |
                    ${private_ssh_key}
              - name: generate public key from private
                shell: ssh-keygen -y -f /root/.ssh/id_rsa > /root/.ssh/id_rsa.pub
                args:
                  creates: /root/.ssh/id_rsa.pub
              - name: enable ssh key for direct root access
                authorized_key:
                  user: root
                  key: "{{ lookup('file', '/root/.ssh/id_rsa.pub') }}"
              - name: check that the dir ${mysql_datadir} exist
                stat:
                  path: ${mysql_datadir}
                register: _mysql_datadir
            when: '"${consul}" != ""'
          - name: get mysql master from consul
            block:
              - uri:
                  url: http://${consul}:${consul_port}/v1/kv/mysql/master/${orchestrator_cluster_name}/hostname
                  method: GET
                  status_code: 200
                  body_format: json
                register: mysql_master_consul
                until: mysql_master_consul.failed == false
                delay: 1
                retries: 6000
              - uri:
                  url: http://${consul}:${consul_port}/v1/kv/mysql/master/${orchestrator_cluster_name}/port
                  method: GET
                  status_code: 200
                  body_format: json
                register: mysql_master_port_consul
                until: mysql_master_port_consul.failed == false
                delay: 1
                retries: 6000
              - set_fact:
                  mysql_master: "{{ mysql_master_consul.json | json_query('[0].Value') | b64decode }}" 
              - set_fact:
                  mysql_master_port: "{{ mysql_master_port_consul.json | json_query('[0].Value') | b64decode }}" 
              - name: wait until ssh and mysql is available on "{{ mysql_master }}:{{ mysql_master_port }}"
                wait_for:
                  host: "{{ mysql_master }}"
                  port: "{{ item }}"
                  timeout: 6000
                with_items:
                  - 22
                  - "{{ mysql_master_port }}"

            when: ( ${bootstrap} == false) and
                  ( '"${consul}" != ""' )
        roles:
          - role: entercloudsuite.mysql-innobackupex
            innobackupex_source_server: "{{ mysql_master }}"
            innobackupex_destination: ${mysql_datadir}
            when: ( ${bootstrap} == false ) and
                  ( _mysql_datadir.stat.exists == false ) and
                  ( "${consul}" != "" )
        post_tasks:
          - name: create mysql user
            user:
              name: mysql
              comment: "MySQL Server"
              shell: /bin/false
              home: /var/lib/mysql
          - name: fix permission
            file: path=/var/lib/mysql recurse=yes owner=mysql group=mysql
          - name: setup client password
            copy:
              dest: /root/.my.cnf
              content: |
                [client]
                user=${mysql_root_name}
                password=${mysql_root_password}

      - name: install mysql
        hosts: 127.0.0.1
        pre_tasks:
          - name: install curl
            package: name=curl
          - name: install percona mysql repo
            shell: curl -O https://repo.percona.com/apt/percona-release_0.1-4.$(lsb_release -sc)_all.deb && dpkg -i percona-release_0.1-4.$(lsb_release -sc)_all.deb
            args:
              creates: /etc/apt/sources.list.d/percona-release.list
              executable: /bin/bash
              chdir: /tmp
          - name: refresh apt cache
            apt:
              update_cache: yes
          - name: create /etc/mysql
            file: path=/etc/mysql state=directory
          - name: add custom mysql config file
            copy:
              dest: custom.cnf
              content: |
                [mysqld]
                enforce_gtid_consistency        = ON
                gtid_mode                       = ON
                report-host                     = ${hostname}.node.${consul_datacenter}.consul
                log-slave-updates               = 1
                relay_log_info_repository       = TABLE
                master_info_repository          = TABLE
        roles:
          - role: entercloudsuite.mysql
            mysql_root_username: ${mysql_root_name}
            mysql_root_password: ${mysql_root_password}
            mysql_packages:
              - percona-server-server-5.7
              - percona-xtrabackup-24
            mysql_replication_master: 127.0.0.1
            mysql_replication_role: master
            mysql_server_id: "{{ 4294967294 | random }}"
            mysql_max_binlog_size: 500M
            mysql_config_include_files:
              - src: custom.cnf
        post_tasks:
          - name: create mysql admin ${mysql_admin_name}
            mysql_user:
              name: ${mysql_admin_name}
              host: '%'
              password: ${mysql_admin_password}
              priv: '*.*:ALL,GRANT'
          - block:
              - name: create mysql user ${mysql_replica_user_name}
                mysql_user:
                  name: ${mysql_replica_user_name}
                  host: '%'
                  password: ${mysql_replica_user_password}
                  priv: '*.*:REPLICATION SLAVE,REPLICATION CLIENT'
              - name: create mysql user ${orchestrator_user}
                mysql_user:
                  name: ${orchestrator_user}
                  host: '%'
                  password: ${orchestrator_password}
                  priv: '*.*:RELOAD,PROCESS,SUPER,REPLICATION SLAVE,REPLICATION CLIENT'
              - name: add permission to read mysql.slave_master_info to mysql user ${orchestrator_user}
                mysql_user:
                  name: ${orchestrator_user}
                  host: '%'
                  password: ${orchestrator_password}
                  priv: 'mysql.slave_master_info:SELECT'
                  append_privs: yes
              - name: add permission to read meta.* to mysql user ${orchestrator_user}
                mysql_user:
                  name: ${orchestrator_user}
                  host: '%'
                  password: ${orchestrator_password}
                  priv: 'meta.*:SELECT'
                  append_privs: yes
            when: '"${consul}" != ""'
          - name: check that the slave coordination file exist
            stat:
              path: ${mysql_datadir}/xtrabackup_binlog_info
            register: _xtrabackup_binlog_info
          - name: set up replica
            block:
              - name: get gtid position
                shell: cat ${mysql_datadir}/xtrabackup_binlog_info | cut -f3
                register: _gtid_position
              - name: reset master status on slave node
                shell: >
                  mysql -uroot -p${mysql_root_password} -e "RESET MASTER"
              - name: set lastest purged gtid
                shell: >
                  mysql -uroot -p${mysql_root_password} -e "SET GLOBAL gtid_purged=\"{{ _gtid_position.stdout }}\";"
              - name: target master server
                shell: >
                  mysql -uroot -p${mysql_root_password} -e "CHANGE MASTER TO MASTER_HOST=\"{{ mysql_master }}\", MASTER_PORT={{ mysql_master_port }}, MASTER_USER=\"${mysql_replica_user_name}\", MASTER_PASSWORD=\"${mysql_replica_user_password}\", MASTER_AUTO_POSITION = 1;"
              - name: start slave
                shell: >
                  mysql -uroot -p${mysql_root_password} -e "START SLAVE\G;"
              - name: delete ${mysql_datadir}/xtrabackup_binlog_info file
                file: path=${mysql_datadir}/xtrabackup_binlog_info state=absent
            when: ( ${bootstrap} == false ) and
                  ( _xtrabackup_binlog_info.stat.exists == True )

      - name: register node
        hosts: 127.0.0.1
        tasks:
          - block:
              - name: register host to orchestrator
                uri:
                  url: http://${orchestrator}:${orchestrator_port}/api/discover/${hostname}.node.${consul_datacenter}.consul/${mysql_port}
                  method: GET
                  status_code: 200
                  body_format: json
                register: register_orchestrator
                until: register_orchestrator.failed == false
                delay: 1
                retries: 6000

              - block:
                  - name: setup meta table
                    shell:
                      cmd: |
                        mysql -uroot -p${mysql_root_password} << EOF
                          CREATE DATABASE IF NOT EXISTS meta;
                          CREATE TABLE IF NOT EXISTS meta.cluster (
                            id int NOT NULL,
                            cluster_name varchar(255),
                            cluster_domain varchar(255),
                            anchor bool,
                            PRIMARY KEY (id)
                          );
                          INSERT INTO meta.cluster(id, cluster_name, anchor) VALUES (1, '${orchestrator_cluster_name}', 1);
                        EOF

                  - name: update consul with master the new cluster
                    shell: |
                      curl -sS http://${orchestrator}:${orchestrator_port}/api/submit-masters-to-kv-stores > /dev/null
                      curl -f -sS http://${consul}:${consul_port}/v1/kv/mysql/master/${orchestrator_cluster_name}/hostname
                    register: update_consul
                    until: ( update_consul.rc == 0 ) and
                       ( update_consul.stdout | from_json | json_query('[0].Value') | b64decode == "${hostname}.node.${consul_datacenter}.consul" )
                    delay: 1
                    retries: 6000
                when: ${bootstrap} == true
            when: '"${consul}" != ""'

      - name: haproxy
        hosts: 127.0.0.1
        roles:
          - role: entercloudsuite.haproxy
            haproxy_user: ${mysql_admin_name}
            haproxy_pass: ${mysql_admin_password}
          - role: entercloudsuite.keepalived
            keepalived_options:
              - name: log-detail
            keepalived_vrrp_scripts:
              chk_haproxy:
                script: '/bin/pidof haproxy'
                weight: 100
                interval: 1

            keepalived_vrrp_instances:
              VI_1:
                interface: "{{ ansible_default_ipv4.interface }}"
                state: "{% if ${number} == 0 %}MASTER{% else %}BACKUP{% endif %}"
                priority: "{{ 110 - ${number}0 }}"
                virtual_router_id: ${mysql_virtual_router_id}

                authentication:
                  auth_type: PASS
                  auth_pass: '${mysql_admin_password}'

                virtual_ipaddresses:
                  - '${mysql_ip}/${mysql_subnet} dev {{ ansible_default_ipv4.interface }} label {{ ansible_default_ipv4.interface }}:1'

                track_scripts:
                  - chk_haproxy

    path: /tmp/cloud/playbook.yml
    permissions: '0400'

  - content: |
      - src: entercloudsuite.mysql-innobackupex
        version: 1.0.0
      - src: entercloudsuite.mysql
        version: 1.0.0
      - src: entercloudsuite.haproxy
        version: 1.1.0
      - src: entercloudsuite.keepalived
        version: 1.0.0
    path: /tmp/cloud/requirements.yml
    permissions: '0400'

  - content: |
      ansible==2.5.1
      asn1crypto==0.23.0
      bcrypt==3.1.4
      cffi==1.11.2
      cryptography==2.1.3
      enum34==1.1.6
      idna==2.6
      ipaddress==1.0.18
      Jinja2==2.10
      MarkupSafe==1.0
      paramiko==2.4.0
      pyasn1==0.3.7
      pycparser==2.18
      pycrypto==2.6.1
      PyNaCl==1.2.0
      PyYAML==3.12
      six==1.11.0
      jmespath==0.9.3
    path: /tmp/cloud/requirements.txt
    permissions: '0400'

runcmd:
  - |
      bash <<'EOF'
      export COMPLETED=false
      while [ "$COMPLETED" == "false" ]; do
        (
          set -e errexit
          set -o pipefail
          # workaround https://github.com/ansible/ansible/issues/21562
          export HOME=/root
          cd /tmp/cloud
          rm -rf bin local share roles include lib || true
          dpkg-query -l libffi-dev || ( apt update -y && apt install libffi-dev -y )
          dpkg-query -l libssl-dev || ( apt update -y && apt install libssl-dev -y )
          test -e /usr/bin/python || ( apt update -y && apt install python-minimal -y )
          test -e /usr/bin/pip || ( apt update -y && apt install python-pip -y )
          test -e /usr/bin/virtualenv || ( apt update -y && apt install virtualenv -y )
          virtualenv .
          source bin/activate
          pip install -r requirements.txt
          mkdir roles || true
          ansible-galaxy install -f -p roles -r requirements.yml
          ansible-playbook -e ansible_python_interpreter=/usr/bin/python --connection=local playbook.yml
        ) >> /var/log/cloud-scripts.log 2>&1
        if [ $? == 0 ]; then
          COMPLETED=true
        fi
        sleep 1
      done
      EOF
